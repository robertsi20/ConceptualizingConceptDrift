{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635c40ef-d1c7-4b44-8cd1-c8c467c916c2",
   "metadata": {},
   "source": [
    "Drift Compression Experiment - See paper Section 4.1 for more details\n",
    "\n",
    "\n",
    "Implementation outline for Experiment\n",
    "1. read in data\n",
    "2. embed data with pretrained model\n",
    "3. run NMF over embeddings and get the nmf representation of the input\n",
    "4. train the drift localizer(DL) on the nmf reps\n",
    "5. estimate global importance of drift localizer\n",
    "6. estimate local importance of each input\n",
    "7. run different h /tilde models over the local importances and compare to DL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82d7ad9-a99a-4cdd-bdd2-aec37d3b5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 13:00:55.636624: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-27 13:00:55.644981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732708855.655562   56208 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732708855.658732   56208 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-27 13:00:55.669662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from concept_helpers.DeepView_Craft import CraftTorchDV as Craft\n",
    "from concept_helpers.DeepView_Craft import CraftTorchSupervised as CraftS\n",
    "from concept_helpers.combined_crafts import CombinedCrafts\n",
    "\n",
    "import urllib.request\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy.sparse.linalg import eigs\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "\n",
    "from xplique.concepts.craft import BaseCraft, DisplayImportancesOrder, Factorization, Sensitivity\n",
    "from sklearn.decomposition import non_negative_factorization\n",
    "from experiment_helpers.helper_function import *\n",
    "from experiment_helpers.driftLocalizer import Localizer\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# loading any timm model\n",
    "model = timm.create_model('nf_resnet50.ra2_in1k', pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# processing\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# cut the model in twop arts (as explained in the paper)\n",
    "# first part is g(.) our 'input_to_latent' model, second part is h(.) our 'latent_to_logit' model\n",
    "g = nn.Sequential(*(list(model.children())[:4]))  # input to penultimate layer\n",
    "h = nn.Sequential(*(list(model.children())[4:]))  # penultimate layer to logits\n",
    "\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt') as f:\n",
    "        imagenet_class_names = np.array(f.read().decode('utf-8').split('\\n'))\n",
    "\n",
    "def gen_images(filelist,folder_names,folder_name2class_id):\n",
    "        for f in filelist:\n",
    "            # print(f)\n",
    "            folder_name = f.split('/')[-2]\n",
    "            if folder_name in folder_names:\n",
    "                # print(folder_name)\n",
    "                class_id = folder_name2class_id[folder_name]\n",
    "                im = Image.open(f)\n",
    "                if len(im.getbands()) == 3:\n",
    "                    yield np.array(im.resize((224, 224))), class_id\n",
    "\n",
    "ood_folder = 'data/ninco_data/NINCO/NINCO_OOD_classes'\n",
    "\n",
    "ood_folder_names = ['french_fries','donuts','waffles','glass_of_milk','cup_cakes','chicken_quesadilla']#, 'donuts'] #'grey_fox', 'Arctic_fox']\n",
    "ood_class_names = ['french_fries','donuts','waffles','glass_of_milk','cup_cakes','chicken_quesadilla']#, 'donuts'] #'grey fox', 'Arctic fox']\n",
    "\n",
    "\n",
    "ood_class_ids = [1001 + i for i,class_name in enumerate(ood_class_names)]\n",
    "ood_folder_name2class_id = dict(zip(ood_folder_names, ood_class_ids))\n",
    "ood_filelist = glob.glob(f'{ood_folder}/*/*.jpg')\n",
    "# print(ood_filelist)\n",
    "\n",
    "ood_images, ood_labels = zip(*gen_images(ood_filelist,ood_folder_names,ood_folder_name2class_id))\n",
    "ood_images, ood_labels = np.array(ood_images), np.array(ood_labels)\n",
    "ood_preprocessed_images = torch.stack([transform(to_pil(img)) for img in ood_images], 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d224f9d-5603-48db-9b35-cdf0a1920f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_maps = []\n",
    "drift_ratios = []\n",
    "drift_localizer = []\n",
    "drift_comparison = []\n",
    "drift_forest = []\n",
    "\n",
    "\n",
    "one_local_one_global = []\n",
    "one_local = []\n",
    "two_local = []\n",
    "three_local = []\n",
    "one_global = []\n",
    "two_global = []\n",
    "three_global = []\n",
    "\n",
    "one_local_one_global_preds = []\n",
    "one_local_preds = []\n",
    "two_local_preds = []\n",
    "three_local_preds = []\n",
    "one_global_preds = []\n",
    "two_global_preds = []\n",
    "three_global_preds = []\n",
    "\n",
    "one_local_one_global_l = []\n",
    "one_local_l = []\n",
    "two_local_l = []\n",
    "three_local_l = []\n",
    "one_global_l = []\n",
    "two_global_l = []\n",
    "three_global_l = []\n",
    "\n",
    "one_local_one_global_preds_l = []\n",
    "one_local_preds_l = []\n",
    "two_local_preds_l = []\n",
    "three_local_preds_l = []\n",
    "one_global_preds_l = []\n",
    "two_global_preds_l = []\n",
    "three_global_preds_l = []\n",
    "\n",
    "# all_local_l = []\n",
    "# all_global_l = []\n",
    "# all_local_preds_l = []\n",
    "# all_global_preds_l = []\n",
    "\n",
    "one_local_l_probs = []\n",
    "two_local_l_probs = []\n",
    "three_local_l_probs = []\n",
    "one_local_preds_l_probs = []\n",
    "two_local_preds_l_probs = []\n",
    "three_local_preds_l_probs = []\n",
    "\n",
    "reconstructed_single_concepts = []\n",
    "reconstructed_single_concepts_preds = []\n",
    "reconstructed_2_concepts = []\n",
    "reconstructed_2_concepts_preds = []\n",
    "reconstructed_3_concepts = []\n",
    "reconstructed_3_concepts_preds = []\n",
    "reconstructed_all_concepts = []\n",
    "reconstructed_all_concepts_preds = []\n",
    "\n",
    "\n",
    "for j in range(50):\n",
    "\n",
    "    sample_ids = np.random.choice(len(ood_preprocessed_images),500, False)\n",
    "    \n",
    "    sample_images = ood_preprocessed_images[sample_ids]\n",
    "    \n",
    "    \n",
    "    # Map each digit to a label indicating whether it occurs before or after the change point, or both, or neither\n",
    "    #  0 - never, 1 - before, 2 - after, 3 - both\n",
    "    \n",
    "    # 0-before, 1-after\n",
    "    import random\n",
    "\n",
    "    # Initialize the label_map keys\n",
    "    keys = ood_class_ids\n",
    "    \n",
    "    # Shuffle the keys for more randomness\n",
    "    random.shuffle(keys)\n",
    "    \n",
    "    # Assign at least one of each label (0, 1, 2)\n",
    "    initial_labels = [0, 1, 2]\n",
    "    random.shuffle(initial_labels)\n",
    "    \n",
    "    # Ensure that the first three keys have 0, 1, and 2 respectively\n",
    "    label_map = {keys[i]: initial_labels[i] for i in range(3)}\n",
    "    \n",
    "    # Randomly assign labels for the remaining keys\n",
    "    for i in range(3, len(keys)):\n",
    "        label_map[keys[i]] = random.randint(0, 2)\n",
    "    \n",
    "    label_maps.append(label_map)\n",
    "\n",
    "    \n",
    "    labels_mapped = np.array([label_map[class_id] for class_id in ood_labels])\n",
    "    \n",
    "    drift_labels = labels_mapped[sample_ids]\n",
    "    \n",
    "    # Randomly assign labels of 1 or 2 to samples with label 3\n",
    "    #  (i.e., digits that occur both before and after the change point)\n",
    "    label_2_idx = np.where(drift_labels == 2)[0]\n",
    "    y_mixed = drift_labels.copy()\n",
    "    y_mixed[label_2_idx] = np.random.choice([0, 1], size=len(label_2_idx))\n",
    "    \n",
    "    sample_labels = y_mixed\n",
    "            \n",
    "    drift_ratios.append({\"BD\": len(np.where(drift_labels == 0)[0]),\n",
    "                         \"AD\": len(np.where(drift_labels == 1)[0]),\n",
    "                         \"Both\": len(np.where(drift_labels == 2)[0])})\n",
    "    \n",
    "    full_size = 256\n",
    "    patch_size= 100\n",
    "    \n",
    "    \n",
    "    #Supervised CRAFT Training\n",
    "    h_craftdv = CraftS(input_to_latent_model=g,\n",
    "                        latent_to_logit_model=h,\n",
    "                        number_of_concepts=5,\n",
    "                        inputs=sample_images,\n",
    "                        labels=sample_labels,\n",
    "                        batch_size=64,\n",
    "                        patch_size=full_size,\n",
    "                        device=device)\n",
    "    \n",
    "    patches, patch_act, train_labels = h_craftdv._extract_patches(sample_images, sample_labels )\n",
    "\n",
    "    bd_indices = np.where(sample_labels != 1)[0]\n",
    "    ad_indices = np.where(sample_labels != 0)[0]\n",
    "\n",
    "    bd_fit = Craft(input_to_latent_model=g,\n",
    "                    latent_to_logit_model=h,\n",
    "                    number_of_concepts=10,\n",
    "                    # labels=h_y,\n",
    "                    patch_size=patch_size,\n",
    "                    batch_size=64,\n",
    "                    device=device)\n",
    "    print(\"Fitting Unsupervised Craft....\")\n",
    "    bd_crops, bd_crops_u, bd_w = bd_fit.fit(sample_images[bd_indices])\n",
    "    \n",
    "    \n",
    "    ad_fit = Craft(input_to_latent_model=g,\n",
    "                        latent_to_logit_model=h,\n",
    "                        number_of_concepts=10,\n",
    "                        # labels=h_y,\n",
    "                        patch_size=patch_size,\n",
    "                        batch_size=64,\n",
    "                        device=device)\n",
    "    print(\"Fitting Unsupervised Craft....\")\n",
    "    ad_crops, ad_crops_u, ad_w = ad_fit.fit(sample_images[ad_indices])\n",
    "\n",
    "    drift_basis = np.vstack([bd_w, ad_w])\n",
    "\n",
    "    drift_craft = CombinedCrafts(input_to_latent_model=g,\n",
    "                    latent_to_logit_model=h,\n",
    "                    number_of_concepts=len(drift_basis),\n",
    "                    inputs=sample_images,\n",
    "                    labels=sample_labels,\n",
    "                    basis = drift_basis,\n",
    "                    batch_size=64,\n",
    "                    patch_size=patch_size,\n",
    "                    device=device)\n",
    "    print(\"Fitting Craft....\")\n",
    "    drift_craft.transform_all()\n",
    "\n",
    "    \n",
    "    X_clean = patch_act\n",
    "    y_clean = train_labels\n",
    "\n",
    "    # Initialize a random forest model with max_leaf_nodes=150\n",
    "    localizer_model = Localizer()\n",
    "    \n",
    "\n",
    "    # Perform the train-test split on X_clean and sample_labels\n",
    "    X_train_clean, X_test_clean, y_train, y_test = \\\n",
    "        train_test_split(X_clean, y_clean, train_size=0.7, random_state=42)\n",
    "\n",
    "    # Fit the model to the mixed set (group 3 is randomly assigned to 1 or 2)\n",
    "    print('Fitting Random Forest classifier...')\n",
    "    localizer_model.fit(X_train_clean, y_train);\n",
    "    print('Fitting complete.')\n",
    "\n",
    "    localizer_bin_preds = localizer_model.l_predict(X_test_clean)\n",
    "    drift_localizer.append(accuracy_score(localizer_bin_preds, y_test))\n",
    "\n",
    "    drift_imp = np.round(estimate_importance_l(localizer_model, drift_craft, drift_basis, X_train_clean),3)\n",
    "\n",
    "\n",
    "    \n",
    "    # y_preds_l, _ = compute_predictions(localizer_model,X_test_clean)\n",
    "    image_drift_imp_l = [estimate_importance_helper_l(drift_craft,localizer_model,drift_basis,\n",
    "                                                  image,class_of_interest=localizer_bin_preds[i]) \n",
    "                               for i,image in enumerate(X_test_clean)]\n",
    "\n",
    "    \n",
    "    \n",
    "    one_local_one_global_l.append(local_one_imp_concept_globally_l(drift_craft,image_drift_imp_l,y_test))\n",
    "    one_local_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=1,labels=y_test))\n",
    "    two_local_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=2,labels=y_test))\n",
    "    three_local_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=3,labels=y_test))\n",
    "    # all_local_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=20,labels=y_test))\n",
    "\n",
    "    one_global_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=1,labels=y_test))\n",
    "    two_global_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=2,labels=y_test))\n",
    "    three_global_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=3,labels=y_test))\n",
    "    # all_global_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=20,labels=y_test))\n",
    "\n",
    "    # all_local_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=20,labels=y_test))\n",
    "\n",
    "    \n",
    "    one_local_one_global_preds_l.append(local_one_imp_concept_globally_l(drift_craft,image_drift_imp_l,localizer_bin_preds))\n",
    "    one_local_preds_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=1,labels=localizer_bin_preds))\n",
    "    two_local_preds_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=2,labels=localizer_bin_preds))\n",
    "    three_local_preds_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=3,labels=localizer_bin_preds))\n",
    "\n",
    "\n",
    "    # all_local_preds_l.append(local_imp_concepts_globally_l(drift_craft,image_drift_imp_l,num=20,labels=localizer_bin_preds))\n",
    "\n",
    "    one_global_preds_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=1,labels=localizer_bin_preds))\n",
    "    two_global_preds_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=2,labels=localizer_bin_preds))\n",
    "    three_global_preds_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=3,labels=localizer_bin_preds))\n",
    "    # all_global_preds_l.append(global_imp_concepts_locally_l(drift_craft,image_drift_imp_l,num=20,labels=localizer_bin_preds))\n",
    "\n",
    "\n",
    "    localizer_bin_train_preds = localizer_model.l_predict(X_train_clean)\n",
    "    image_drift_imp_l_train = [estimate_importance_helper_l(drift_craft,localizer_model,drift_basis,\n",
    "                                                  image,class_of_interest=localizer_bin_train_preds[i]) \n",
    "                               for i,image in enumerate(X_train_clean)]\n",
    "    concept_dist = concept_counter(image_drift_imp_l_train,localizer_bin_train_preds)\n",
    "\n",
    "    one_local_l_probs.append(local_imp_concepts_probability(concept_dist,image_drift_imp_l,num=1,labels=y_test))\n",
    "    two_local_l_probs.append(local_imp_concepts_probability(concept_dist,image_drift_imp_l,num=2,labels=y_test))\n",
    "    three_local_l_probs.append(local_imp_concepts_probability(concept_dist,image_drift_imp_l,num=3,labels=y_test))\n",
    "\n",
    "    one_local_preds_l_probs.append(local_imp_concepts_probability(concept_dist,image_drift_imp_l,num=1,labels=localizer_bin_preds))\n",
    "    two_local_preds_l_probs.append(local_imp_concepts_probability(concept_dist,image_drift_imp_l,num=2,labels=localizer_bin_preds))\n",
    "    three_local_preds_l_probs.append(local_imp_concepts_probability(concept_dist,image_drift_imp_l,num=3,labels=localizer_bin_preds))\n",
    "\n",
    "    reconstructed_single_concept = reconstruct_inputs(X_test_clean, image_drift_imp_l, drift_basis, num_concepts=1)\n",
    "    localizer_preds = localizer_model.l_predict(reconstructed_single_concept)\n",
    "    reconstructed_single_concepts.append(accuracy_score(localizer_preds, y_test))\n",
    "    reconstructed_single_concepts_preds.append(accuracy_score(localizer_preds, localizer_bin_preds))\n",
    "\n",
    "\n",
    "    reconstructed_2_concept = reconstruct_inputs(X_test_clean, image_drift_imp_l, drift_basis, num_concepts=2)\n",
    "    localizer_preds = localizer_model.l_predict(reconstructed_2_concept)\n",
    "    reconstructed_2_concepts.append(accuracy_score(localizer_preds, y_test))\n",
    "    reconstructed_2_concepts_preds.append(accuracy_score(localizer_preds, localizer_bin_preds))\n",
    "    \n",
    "    reconstructed_3_concept = reconstruct_inputs(X_test_clean, image_drift_imp_l, drift_basis, num_concepts=3)\n",
    "    localizer_preds = localizer_model.l_predict(reconstructed_3_concept)\n",
    "    reconstructed_3_concepts.append(accuracy_score(localizer_preds, y_test))\n",
    "    reconstructed_3_concepts_preds.append(accuracy_score(localizer_preds, localizer_bin_preds))\n",
    "\n",
    "\n",
    "    reconstructed_all_concept = reconstruct_inputs(X_test_clean, image_drift_imp_l, drift_basis, num_concepts=21)\n",
    "    localizer_preds = localizer_model.l_predict(reconstructed_all_concept)\n",
    "    reconstructed_all_concepts.append(accuracy_score(localizer_preds, y_test))\n",
    "    reconstructed_all_concepts_preds.append(accuracy_score(localizer_preds, localizer_bin_preds))\n",
    "\n",
    "    print(\"Run:\",j)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f11da34-8032-4692-9444-1d136d0b133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Example accuracy lists (replace with your actual data)\n",
    "# method_1 = [0.85, 0.87, 0.86, ...]  # List of 50 accuracies for method 1\n",
    "# method_2 = [0.82, 0.84, 0.83, ...]  # List of 50 accuracies for method 2\n",
    "# Repeat for all 7 methods\n",
    "# methods = [drift_localizer,one_local_one_global, one_local,two_local,\n",
    "#            three_local,one_global,two_global,three_global,one_local_one_global_p,\n",
    "#             one_local_p, two_local_p ,three_local_p ,one_global_p ,two_global_p,\n",
    "#             three_global_p]  # List of lists\n",
    "\n",
    "# methods = [drift_localizer,one_local_one_global_p,\n",
    "#             one_local_p, two_local_p ,three_local_p ,one_global_p ,two_global_p,\n",
    "#             three_global_p]\n",
    "\n",
    "# one_local_l_probs = []\n",
    "# two_local_l_probs = []\n",
    "# three_local_l_probs = []\n",
    "# one_local_preds_l_probs = []\n",
    "# two_local_preds_l_probs = []\n",
    "# three_local_preds_l_probs = []\n",
    "\n",
    "methods = [ drift_localizer,\n",
    "            one_local_one_global_l,\n",
    "            one_local_l,\n",
    "            two_local_l,\n",
    "            three_local_l,\n",
    "            one_local_l_probs,\n",
    "            two_local_l_probs,\n",
    "            three_local_l_probs,\n",
    "        \n",
    "            one_global_l,\n",
    "            two_global_l,\n",
    "            three_global_l,\n",
    "       \n",
    "            one_local_one_global_preds_l,\n",
    "            one_local_preds_l,\n",
    "            two_local_preds_l,\n",
    "            three_local_preds_l,\n",
    "           one_local_preds_l_probs,\n",
    "            two_local_preds_l_probs,\n",
    "            three_local_preds_l_probs,\n",
    "           \n",
    "            one_global_preds_l,\n",
    "            two_global_preds_l,\n",
    "            three_global_preds_l,\n",
    "\n",
    "           reconstructed_single_concepts,\n",
    "reconstructed_single_concepts_preds,\n",
    "reconstructed_2_concepts,\n",
    "reconstructed_2_concepts_preds,\n",
    "reconstructed_3_concepts,\n",
    "reconstructed_3_concepts_preds,\n",
    "reconstructed_all_concepts,\n",
    "reconstructed_all_concepts_preds,\n",
    "           \n",
    "          \n",
    "            # drift_comparison,\n",
    "            # drift_forest,\n",
    "            # one_local_one_global,\n",
    "            # one_local,\n",
    "            # two_local,\n",
    "            # three_local,\n",
    "            # one_global,\n",
    "            # two_global,\n",
    "            # three_global,\n",
    "            # one_local_one_global_preds,\n",
    "            # one_local_preds,\n",
    "            # two_local_preds,\n",
    "            # three_local_preds,\n",
    "            # one_global_preds,\n",
    "            # two_global_preds,\n",
    "            # three_global_preds,\n",
    "            label_maps,\n",
    "            drift_ratios]\n",
    "\n",
    "# method_names = ['Drift Localizer', '1_Local_global','1_Local', '2_Local', '3_Local', '1_Global', '2_Global', '3_Global',\n",
    "#                '1_Local_global_p','1_Local_p', '2_Local_p', '3_Local_p', '1_Global_p', '2_Global_p', '3_Global_p']\n",
    "# method_names = ['Drift Localizer', \n",
    "#                '1_Local_global','1_Local', '2_Local', '3_Local', '1_Global', '2_Global', '3_Global',\n",
    "#                '1_Local_global_preds','1_Local_preds', '2_Local_preds', '3_Local_preds', '1_Global_preds', '2_Global_preds', '3_Global_preds']\n",
    "\n",
    "method_names = [ \"drift_localizer\",            \n",
    "            \"one_local_one_global_l\",\n",
    "            \"one_local_l\",\n",
    "            \"two_local_l\",\n",
    "            \"three_local_l\",\n",
    "                \"one_local_l_probs\",\n",
    "            \"two_local_l_probs\",\n",
    "            \"three_local_l_probs\",\n",
    "            \n",
    "            \"one_global_l\",\n",
    "            \"two_global_l\",\n",
    "            \"three_global_l\",\n",
    "        \n",
    "            \"one_local_one_global_preds_l\",\n",
    "            \"one_local_preds_l\",\n",
    "            \"two_local_preds_l\",\n",
    "            \"three_local_preds_l\",\n",
    "            \"one_local_preds_l_probs\",\n",
    "            \"two_local_preds_l_probs\",\n",
    "            \"three_local_preds_l_probs\",\n",
    "    \n",
    "            \"one_global_preds_l\",\n",
    "            \"two_global_preds_l\",\n",
    "            \"three_global_preds_l\",\n",
    "                \"reconstructed_single_concepts\",\n",
    "            \"reconstructed_single_concepts_preds\",\n",
    "            \"reconstructed_2_concepts\",\n",
    "            \"reconstructed_2_concepts_preds\",\n",
    "            \"reconstructed_3_concepts\",\n",
    "            \"reconstructed_3_concepts_preds\",\n",
    "            \"reconstructed_all_concepts\",\n",
    "            \"reconstructed_all_concepts_preds\",\n",
    "            # \"drift_comparison\",\n",
    "            #     \"drift_forest\",\n",
    "            # \"one_local_one_global\",\n",
    "            # \"one_local\",\n",
    "            # \"two_local\",\n",
    "            # \"three_local\",\n",
    "            # \"one_global\",\n",
    "            # \"two_global\",\n",
    "            # \"three_global\",\n",
    "            # \"one_local_one_global_preds\",\n",
    "            # \"one_local_preds\",\n",
    "            # \"two_local_preds\",\n",
    "            # \"three_local_preds\",\n",
    "            # \"one_global_preds\",\n",
    "            # \"two_global_preds\",\n",
    "            # \"three_global_preds\",\n",
    "               \"label_maps\",\n",
    "               \"drift_ratios\"]\n",
    "\n",
    "# Write to CSV\n",
    "with open('paper_experiment_reproduce.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Method'] + [f'Run_{i+1}' for i in range(50)])  # Header row\n",
    "    for method, accuracies in zip(method_names, methods):\n",
    "        writer.writerow([method] + accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a35747f-9cb7-42bf-89d6-3c75054d4069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drift_localizer': (0.8148000000000001, 0.06316525064375894), 'one_local_one_global_l': (0.7786666666666667, 0.058575687030636786), 'one_local_l': (0.7885333333333335, 0.05674175417333048), 'two_local_l': (0.7698666666666667, 0.06158268154819719), 'three_local_l': (0.7452, 0.06062328118983848), 'one_local_l_probs': (0.7878666666666667, 0.05545272260623859), 'two_local_l_probs': (0.772, 0.05845416057808792), 'three_local_l_probs': (0.7484000000000001, 0.05791656834524028), 'one_global_l': (0.6836000000000001, 0.07112067991176062), 'two_global_l': (0.7294666666666666, 0.06584952543488828), 'three_global_l': (0.7446666666666666, 0.06060803027102378), 'one_local_one_global_preds_l': (0.7902666666666666, 0.10004630039247939), 'one_local_preds_l': (0.8206666666666667, 0.08527341646466122), 'two_local_preds_l': (0.7980000000000002, 0.0904433524367601), 'three_local_preds_l': (0.7786666666666666, 0.08652295777550732), 'one_local_preds_l_probs': (0.8290666666666668, 0.08494453092067396), 'two_local_preds_l_probs': (0.8134666666666667, 0.0804375810454569), 'three_local_preds_l_probs': (0.7909333333333334, 0.07416210472861309), 'one_global_preds_l': (0.7034666666666667, 0.09508115831576026), 'two_global_preds_l': (0.7610666666666668, 0.08966465920925094), 'three_global_preds_l': (0.7711999999999999, 0.0858467885893882), 'reconstructed_single_concepts': (0.7637333333333332, 0.06211330149188837), 'reconstructed_single_concepts_preds': (0.85, 0.07315432697766795), 'reconstructed_2_concepts': (0.7694666666666667, 0.07015018808876346), 'reconstructed_2_concepts_preds': (0.8695999999999999, 0.07611873765520696), 'reconstructed_3_concepts': (0.7716, 0.06811506604432109), 'reconstructed_3_concepts_preds': (0.8757333333333333, 0.07858481617547587), 'reconstructed_all_concepts': (0.7726666666666666, 0.07058170993804116), 'reconstructed_all_concepts_preds': (0.8887999999999999, 0.08022678965144858)}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV\n",
    "# df = pd.read_csv('new_experiments_run5_only_bdad.csv')\n",
    "\n",
    "df = pd.read_csv('paper_experiment_D1.csv')\n",
    "df = df.iloc[:29]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "stats = {}\n",
    "for method in df['Method']:\n",
    "    accuracies = df[df['Method'] == method].drop('Method', axis=1).values.flatten().astype(float)\n",
    "    mean = np.mean(accuracies)\n",
    "    # median = np.median(accuracies)\n",
    "    \n",
    "    std = np.std(accuracies)\n",
    "    # stats[method] = (mean, std, median)\n",
    "    stats[method] = (mean, std)\n",
    "# Example output for stats\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873b43f3-4809-4feb-8990-dc5b28c3aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[h!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c}\n",
      "\\hline\n",
      "Method & Accuracy (Mean ± Std Dev) \\\\\n",
      "\\hline\n",
      "drift_localizer & 0.815 ± 0.063 \\\\ \n",
      "one_local_one_global_l & 0.779 ± 0.059 \\\\ \n",
      "one_local_l & 0.789 ± 0.057 \\\\ \n",
      "two_local_l & 0.770 ± 0.062 \\\\ \n",
      "three_local_l & 0.745 ± 0.061 \\\\ \n",
      "one_local_l_probs & 0.788 ± 0.055 \\\\ \n",
      "two_local_l_probs & 0.772 ± 0.058 \\\\ \n",
      "three_local_l_probs & 0.748 ± 0.058 \\\\ \n",
      "one_global_l & 0.684 ± 0.071 \\\\ \n",
      "two_global_l & 0.729 ± 0.066 \\\\ \n",
      "three_global_l & 0.745 ± 0.061 \\\\ \n",
      "one_local_one_global_preds_l & 0.790 ± 0.100 \\\\ \n",
      "one_local_preds_l & 0.821 ± 0.085 \\\\ \n",
      "two_local_preds_l & 0.798 ± 0.090 \\\\ \n",
      "three_local_preds_l & 0.779 ± 0.087 \\\\ \n",
      "one_local_preds_l_probs & 0.829 ± 0.085 \\\\ \n",
      "two_local_preds_l_probs & 0.813 ± 0.080 \\\\ \n",
      "three_local_preds_l_probs & 0.791 ± 0.074 \\\\ \n",
      "one_global_preds_l & 0.703 ± 0.095 \\\\ \n",
      "two_global_preds_l & 0.761 ± 0.090 \\\\ \n",
      "three_global_preds_l & 0.771 ± 0.086 \\\\ \n",
      "reconstructed_single_concepts & 0.764 ± 0.062 \\\\ \n",
      "reconstructed_single_concepts_preds & 0.850 ± 0.073 \\\\ \n",
      "reconstructed_2_concepts & 0.769 ± 0.070 \\\\ \n",
      "reconstructed_2_concepts_preds & 0.870 ± 0.076 \\\\ \n",
      "reconstructed_3_concepts & 0.772 ± 0.068 \\\\ \n",
      "reconstructed_3_concepts_preds & 0.876 ± 0.079 \\\\ \n",
      "reconstructed_all_concepts & 0.773 ± 0.071 \\\\ \n",
      "reconstructed_all_concepts_preds & 0.889 ± 0.080 \\\\ \n",
      "\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Accuracy of different methods}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_table = \"\"\"\n",
    "\\\\begin{table}[h!]\n",
    "\\\\centering\n",
    "\\\\begin{tabular}{l|c}\n",
    "\\\\hline\n",
    "Method & Accuracy (Mean ± Std Dev) \\\\\\\\\n",
    "\\\\hline\n",
    "\"\"\"\n",
    "\n",
    "for method, (mean, std) in stats.items():\n",
    "    latex_table += f\"{method} & {mean:.3f} ± {std:.3f} \\\\\\\\ \\n\"\n",
    "\n",
    "latex_table += \"\"\"\n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\caption{Accuracy of different methods}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#include all and see what happens\n",
    "\n",
    "# Output the LaTeX table\n",
    "print(latex_table)\n",
    "\n",
    "##Model h tilde for paper is encompassed by \"one_local_l_probs\"\n",
    "\n",
    "## We have other models here which use more concepts for possible future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8728c-c8ac-480c-839d-e9b8993698e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
